# model config
model_config:
    encoder_layers: 12
    encoder_num_heads: 12
    encoder_dim: 768
    mlp_ratio: 4

# context config
seed: 2022
use_parallel: False
context:
    mode: 0 #0--Graph Mode; 1--Pynative Mode
    device_target: "Ascend"
    max_call_depth: 10000
    save_graphs: False
    device_id: 7

# dataset base
dataset_name: "imagenet"
eval_engine: 'imagenet'

# train dataset
dataset_path: "/mnt/vision/ImageNet1K/CLS-LOC/train"
train_num_workers: 14
interpolation: "BILINEAR"
train_image_size: 224
autoaugment: 1
crop_min: 0.2
mixup: 0.8

# eval datasets
eval_path: '/mnt/vision/ImageNet1K/CLS-LOC/val'
eval_image_size: 224
eval_batch_size: 256
eval_interval: 1
eval_offset: -1
eval_num_workers: 12

# train config
epoch: 300
batch_size: 1
patch_size: 32
sink_mode: True
dropout: 0.1
num_classes: 1001
per_step_size: 0
finetune_ckpt: "vit-base-p32/VIT-B-P32-300_625.ckpt"

# loss
loss_scale: 1024
use_label_smooth: 1
label_smooth_factor: 0.1
loss_name: "ce_smooth_mixup"

# optimizer
beta1: 0.9
beta2: 0.95
weight_decay: 0.3

# lr sechdule
start_learning_rate: 0.0016
end_learning_rate: 0.0
warmup_epochs: 20

# callback
save_ckpt_epochs: 10
ckpt_save_dir: "/cache/ckpt"
prefix: "VIT-B-P32"
obs_dir: "obs://muti-modal/vit-base-p32/lr1.6e-3"


