{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710b42b9-4b01-4fac-9956-16ec3535d097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://192.168.9.66:8888/repository/pypi/simple\n",
      "Collecting transformers\n",
      "  Downloading http://192.168.9.66:8888/repository/pypi/packages/6a/4a/d35a2140bba25a26b8c8daf74b89f3ab46ad957e998bf77b7b4305187bc5/transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading http://192.168.9.66:8888/repository/pypi/packages/94/b3/ff2845971788613e646e667043fdb5f128e2e540aefa09a3c55be8290d6d/filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading http://192.168.9.66:8888/repository/pypi/packages/03/78/8ae719924560be4b8513b50e6af4bc76d7e71fa00e6ffcdff03a3d152f44/tokenizers-0.12.1-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading http://192.168.9.66:8888/repository/pypi/packages/10/38/e3d6e93f7aa282882f351ab0d9986f1b30f23143b96b5a21850045d24d17/regex-2022.9.11-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (748 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m748.5/748.5 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from transformers) (1.21.2)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading http://192.168.9.66:8888/repository/pypi/packages/8a/27/d92a2d41373fc91045d9a2ba5f9e0664a0f1ba6c3b52d9bc40ff1eccb5be/huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from transformers) (4.46.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Installing collected packages: tokenizers, regex, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.8.0 huggingface-hub-0.9.1 regex-2022.9.11 tokenizers-0.12.1 transformers-4.21.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c195a1b8-627d-466c-a493-97578257f3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "09/13/2022 09:57:52 - INFO - __main__ -  load model: /home/ma-user/work/deploy_vqa/opt_vqa/opt_vqa_graph.mindir\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start OPT vqa inference....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/13/2022 09:58:32 - INFO - __main__ -  warmup network...\n",
      "09/13/2022 10:17:51 - INFO - __main__ -  warmup network successfully!\n",
      "09/13/2022 10:17:51 - INFO - __main__ -  load network successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init time: 1198.2748715877533\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "import os\n",
    "from threading import Thread\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "os.environ[\"GLOG_v\"] = \"3\"\n",
    "os.environ[\"ASCEND_GLOBAL_LOG_LEVEL\"] = \"3\"\n",
    "from mindspore import context, Tensor, Model, nn, load\n",
    "from mindspore.dataset.vision.utils import Inter\n",
    "import mindspore.dataset.vision.c_transforms as C\n",
    "\n",
    "_LOG_FMT = '%(asctime)s - %(levelname)s - %(name)s -  %(message)s'\n",
    "_DATE_FMT = '%m/%d/%Y %H:%M:%S'\n",
    "logging.basicConfig(format=_LOG_FMT, datefmt=_DATE_FMT, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")\n",
    "\n",
    "\n",
    "def pad_sequence(sequences, batch_first=True, padding_value=0.0, max_lens=-1):\n",
    "    \"\"\"pad_sequence\"\"\"\n",
    "    lens = [len(x) for x in sequences]\n",
    "    if max_lens == -1:\n",
    "        max_lens = max(lens)\n",
    "\n",
    "    padded_seq = []\n",
    "    for x in sequences:\n",
    "        pad_width = [(0, max_lens - len(x))]\n",
    "        padded_seq.append(np.pad(x, pad_width, constant_values=(padding_value, padding_value)))\n",
    "\n",
    "    sequences = np.stack(padded_seq, axis=0 if batch_first else 1)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def pad_tensors(tensors, lens=None, pad=0, max_len=-1):\n",
    "    \"\"\"B x [T, ...]\"\"\"\n",
    "    if lens is None:\n",
    "        lens = [t.shape[0] for t in tensors]\n",
    "    if max_len == -1:\n",
    "        max_len = max(lens)\n",
    "    bs = len(tensors)\n",
    "    hid = tensors[0].shape[-1]\n",
    "    dtype = tensors[0].dtype\n",
    "    output = np.zeros((bs, max_len, hid), dtype=dtype)\n",
    "    if pad:\n",
    "        output.fill(pad)\n",
    "    for i, (t, l) in enumerate(zip(tensors, lens)):\n",
    "        output[i, :l, ...] = t\n",
    "    return output\n",
    "\n",
    "\n",
    "def decode_sequence(ix_to_word, seq, split=' '):\n",
    "    \"\"\"\n",
    "    decode_sequence\n",
    "    \"\"\"\n",
    "    N = seq.shape[0]\n",
    "    D = seq.shape[1]\n",
    "    out = []\n",
    "    for i in range(N):\n",
    "        txt = ''\n",
    "        for j in range(D):\n",
    "            ix = seq[i, j]\n",
    "            if ix > 0:\n",
    "                if j >= 1:\n",
    "                    txt = txt + split\n",
    "                txt = txt + ix_to_word[str(ix.item())]\n",
    "            else:\n",
    "                break\n",
    "        out.append(txt.replace(' ##', ''))\n",
    "    return out\n",
    "\n",
    "\n",
    "class opt_vqa_inference:\n",
    "    def __init__(self, model_path, model_name, vocab_name, bert_base_chinese_vocab):\n",
    "        self.image_size = 448\n",
    "        self.patch_size = 32\n",
    "\n",
    "        resize = self.image_size\n",
    "        image_size = self.image_size\n",
    "        mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "        std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "        interpolation = \"BILINEAR\"\n",
    "        if hasattr(Inter, interpolation):\n",
    "            interpolation = getattr(Inter, interpolation)\n",
    "        else:\n",
    "            interpolation = Inter.BILINEAR\n",
    "            logger.warning('cannot find interpolation_type: {}, use {} instead'.format(interpolation, 'BILINEAR'))\n",
    "        self.trans = [\n",
    "            C.Resize(resize, interpolation=interpolation),\n",
    "            C.CenterCrop(image_size),\n",
    "            C.Normalize(mean=mean, std=std),\n",
    "            C.HWC2CHW()\n",
    "        ]\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.model_name = model_name\n",
    "        self.vocab_name = vocab_name\n",
    "        self.bert_base_chinese_vocab = bert_base_chinese_vocab\n",
    "        model = os.path.join(self.model_path, self.model_name)\n",
    "        logger.info(f\"load model: {model}\")\n",
    "        self.graph = load(model)\n",
    "        self.network = nn.GraphCell(self.graph)\n",
    "        self.model = Model(self.network)\n",
    "        vocab = os.path.join(self.model_path, self.vocab_name)\n",
    "        self.vocab = json.load(open(vocab))\n",
    "        bert_base_chinese_vocab = os.path.join(self.model_path, self.bert_base_chinese_vocab)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_base_chinese_vocab)\n",
    "        # 模型预热，否则首次推理的时间会很长\n",
    "        # self.load = Thread(target=self._warmup)\n",
    "        # self.load.start()\n",
    "        self._warmup()\n",
    "        logger.info(\"load network successfully!\")\n",
    "\n",
    "    def _warmup(self):\n",
    "        from mindspore import float32, int64\n",
    "        logger.info(\"warmup network...\")\n",
    "        input_ids = Tensor(np.array(np.random.randn(1, 50), dtype=np.float32), int64)\n",
    "        position_ids = Tensor(np.expand_dims(np.arange(0, input_ids.shape[1], dtype=np.int64), 0), int64)\n",
    "        image = Tensor(np.array(np.random.randn(1, 197, 3072), dtype=np.float32), float32)\n",
    "        img_pos_feat = Tensor(np.expand_dims(np.arange(0, 197, dtype=np.int64), axis=0), int64)\n",
    "        attn_masks = Tensor(np.ones((1, 247), dtype=np.int64), int64)\n",
    "        gather_index = Tensor(np.expand_dims(np.arange(0, 247, dtype=np.int64), axis=0), int64)\n",
    "        self.model.predict(input_ids, position_ids, image, img_pos_feat, attn_masks, gather_index)\n",
    "        logger.info(\"warmup network successfully!\")\n",
    "\n",
    "    def preprocess(self, image, text):\n",
    "        from mindspore import float32, int64\n",
    "        print(\"preprocess===================================================================\\n\")\n",
    "        image = Image.open(image).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        for tran in self.trans:\n",
    "            image = tran(image)\n",
    "\n",
    "        p = self.patch_size\n",
    "        channels, h, w = image.shape\n",
    "        x = np.reshape(image, (channels, h // p, p, w // p, p))\n",
    "        x = np.transpose(x, (1, 3, 0, 2, 4))\n",
    "        patches = np.reshape(x, ((h // p) * (w // p), channels * p * p))\n",
    "        img_pos_feat = np.arange(patches.shape[0] + 1)\n",
    "        attn_masks = np.ones(img_pos_feat.shape[0], dtype=np.int64)\n",
    "\n",
    "        img_feat = Tensor(pad_tensors([patches, ], [196], max_len=197))\n",
    "        img_pos_feat = Tensor(np.stack([img_pos_feat, ], axis=0))\n",
    "        attn_masks = Tensor(pad_sequence([attn_masks, ], batch_first=True, padding_value=0, max_lens=247))\n",
    "        out_size = attn_masks.shape[1]\n",
    "        batch_size = attn_masks.shape[0]\n",
    "        gather_index = Tensor(np.expand_dims(np.arange(0, out_size, dtype=np.int64), 0).repeat(batch_size, axis=0))\n",
    "\n",
    "        question_tokens = self.tokenizer.tokenize(text)\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(question_tokens)\n",
    "        # print(\"tokenizer input_ids: \\n\", input_ids)\n",
    "        input_ids = Tensor(pad_sequence([input_ids, ], batch_first=True, padding_value=0, max_lens=50), int64)\n",
    "        # print(\"padding input_ids: \\n\", input_ids)\n",
    "        position_ids = Tensor(np.expand_dims(np.arange(0, input_ids.shape[1], dtype=np.int64), 0), int64)\n",
    "        # print(\"position_ids: \", position_ids)\n",
    "         \n",
    "        return input_ids, position_ids, img_feat, img_pos_feat, attn_masks, gather_index\n",
    "\n",
    "    def postprocess(self, sequence):\n",
    "        print(\"postprocess=================================================================\\n\")\n",
    "        # print(\"sequence: \\n\", sequence)\n",
    "        return decode_sequence(self.vocab, sequence[:, 0, 1:].asnumpy(), split='')\n",
    "\n",
    "    def inference(self, input_data):\n",
    "        # 阻塞预热\n",
    "        # self.load.join()\n",
    "        inference_result = {}\n",
    "        for k, v in input_data.items():\n",
    "            instance_result = {}\n",
    "            (input_ids, position_ids, image, img_pos_feat, attn_masks, gather_index) = self.preprocess(v[\"image\"], v[\"question\"])\n",
    "            sequence = self.model.predict(input_ids, position_ids, image, img_pos_feat, attn_masks, gather_index)\n",
    "            result = self.postprocess(sequence)\n",
    "            # print(\"result: \", result)\n",
    "            # for file_name, file_content in v.items():\n",
    "            #     (input_ids, position_ids, image, img_pos_feat, attn_masks, gather_index) = self.preprocess(file_content[\"image\"], file_content[\"question\"])\n",
    "            #     sequence = self.model.predict(input_ids, position_ids, image, img_pos_feat, attn_masks, gather_index)\n",
    "            #     instance_result[file_name] = self.postprocess(sequence)\n",
    "            inference_result[k] = result[0]\n",
    "        return inference_result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    import io\n",
    "\n",
    "    print(\"start OPT vqa inference....\")\n",
    "    # model_path = os.path.split(__file__)[0]\n",
    "    model_path = \"/home/ma-user/work/deploy_vqa/opt_vqa\"\n",
    "    model_name = \"opt_vqa_graph.mindir\"\n",
    "    vocab_name = \"data/ids_to_tokens_zh.json\"\n",
    "    bert_base_chinese_vocab = \"data/bert-base-chinese-vocab.txt\"\n",
    "\n",
    "    last_time = time.time()\n",
    "    inference_object = opt_vqa_inference(model_path, model_name, vocab_name, bert_base_chinese_vocab)\n",
    "    print(\"init time:\", time.time() - last_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4138e9b-3ba4-4a7c-b796-628135276069",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87735e958c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xiaopan.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mbyte_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BMP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "path1 = \"COCO_val2014_000000019608.jpg\"\n",
    "path2 = \"test.png\"\n",
    "path3 = \"xiaopan.png\"\n",
    "with Image.open(path2) as img:\n",
    "    byte_stream = io.BytesIO()\n",
    "    img.save(byte_stream, format='BMP')\n",
    "print(\"byte_stream: \", byte_stream)\n",
    "Image.open(byte_stream).show()\n",
    "\n",
    "question = \"后面的牌子是什么？\"\n",
    "# preprocess_result = inference_object.preprocess(byte_stream, question)\n",
    "# print(\"preprocess result:\", preprocess_result)\n",
    "input_data = {\"instances\": {\"image\": byte_stream, \"question\": question}}\n",
    "last_time = time.time()\n",
    "inference_result = inference_object.inference(input_data)\n",
    "print(\"inference time:\", time.time() - last_time)\n",
    "print(\"Q: \", question)\n",
    "print(\"A: \", inference_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
