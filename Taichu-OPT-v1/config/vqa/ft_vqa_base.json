{
    "model_config": "config/vqa/cross_modal_encoder_base.json",
    "train_batch_size": 10,
    "val_batch_size": 10,
    "gradient_accumulation_steps": 1,
    "valid_steps": 1000,
    "decay_steps": 100000,
    "optim": "adamw",
    "use_vit": true,
    "use_patch": true,
    "beam_width": 1,
    "image_size": 448,
    "patch_size": 32,
    "vit_type": 0,
    "betas": [
        0.9,
        0.98
    ],
    "dropout": 0.1,
    "weight_decay": 0.01,
    "grad_norm": 5.0,
    "warmup_steps": 1000,
    "seed": 42,
    "fp16": true,
    "n_workers": 12,
    "pin_mem": true,
    "vocab_path": "dataset/vqa/ids_to_tokens_zh.json",
    "data_type": 2,
    "show_time": true,
    "name_txt": "FM-IQA.json",
    "mode": "train",
    "ids_train_path": "",
    "train_datasets": [
        {
            "name": "coco",
            "db": [
                "dataset/vqa/txt"
            ],
            "img": [
                "dataset/vqa/img"
            ],
            "tasks": [
                "vqa"
            ],
            "mix_ratio": [
                1
            ]
        }
    ],
    "vqa_eval_gt" : "dataset/vqa/txt/FM-IQA.json",
    "ids_val_path": "",
    "val_datasets": [
        {
            "name": "coco",
            "db": [
                "dataset/vqa/txt"
            ],
            "img": [
                "dataset/vqa/img"
            ],
            "tasks": [
                "vqa"
            ]
        }
    ]
}
